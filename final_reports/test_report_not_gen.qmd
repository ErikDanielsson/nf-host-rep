---
title: "TreePPL host repertoire"
format:
  html:
    code-fold: true
  pdf:
    code-fold: false
    code-tools: false
    echo: false
    fig-width: 6
    fig-height: 4
execute:
  freeze: auto
jupyter: phylogenetics
---
```{python}

# Add the python helper directory to the path

import sys
import os
import shutil
from pathlib import Path
from functools import reduce

CWD = Path(os.getcwd())
sys.path.append(str(CWD / "../python_helpers/"))

# Import the python helpers
import proc_output
from parse_trace import parse_trace

import arviz as az
import xarray as xr
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import warnings
import pandas as pd
import numpy as np
import seaborn as sns

# Suppress warning from arviz
# -- it complains that all samples are the same
warnings.filterwarnings(action="ignore", module=r"arviz")

# Set global script options, the rest will follow from these
RUN_NAME = "final-tppl-comp"
CACHE = True  # Create a cache of the files to speed up next render?
CLEAR_CACHE = False  # Clear the cache from a previous render?

# Check if we should clear the cache
if not CACHE or CLEAR_CACHE:
    proc_output.clear_temp_dir(tempdir_suffix=RUN_NAME)

# Set the names of the output directories
glob_outdir = CWD / ".." / "simulations"
outdir = glob_outdir / RUN_NAME
datadir = outdir / "data"
simdir = outdir / "sims"
bindir = outdir / "bins"
trace = outdir / "final-tppl-comp.trace.txt"
compile_params_path = bindir / "compile_id_to_configuration.csv"
data_params_path = datadir / "param_id_to_configuration.csv"

```
```{python}

# Read files
df = proc_output.get_files_in_dir(
    simdir,
    {
        "tppl": proc_output.get_tppl_output_pattern(),
        "rb": proc_output.get_rb_output_pattern(),
    },
)
df_fn = df.copy()

df = proc_output.create_inference_data_df(
    df,
    {
        "tppl": lambda fn: proc_output.read_tppl_incremental_file(
            fn, with_file=CACHE, tempdir_suffix=RUN_NAME
        ),
        "rb": lambda fn: proc_output.read_rb_file(
            fn, with_file=CACHE, tempdir_suffix=RUN_NAME
        ),
    },
    1000,
    1,
    end=9000,
)
dfs = {k: v for k, v in df.groupby("file_type")}

has_rb = "rb" in dfs
has_tppl = "tppl" in dfs
if has_rb:
    df_rb = dfs["rb"]

if has_tppl:
    df_tppl = dfs["tppl"]

```

The models were run on dataset generated with the following parameters
```{python}

data_params = proc_output.parse_data_params(data_params_path)
data_params = data_params.sort_values(by="param_id")

```

### TreePPL compile params
The following TreePPL models were compiled
```{python}

compile_params = proc_output.parse_compile_params(compile_params_path)
compile_params

```
### Nextflow trace
The trace file contains status and process time
```{python}
trace_df = parse_trace(trace)
```

```{python}
df_tppl_with_compile_params = proc_output.add_compile_params(df_tppl, compile_params)
df = pd.concat([df_rb, df_tppl_with_compile_params])
fillna_cols = ["model_name", "model_dir"]
df[fillna_cols] = df[fillna_cols].fillna("rb")
```
```{python}

timed_df = pd.merge(
    left=trace_df,
    right=df,
    how="left",
    left_on=["genid", "param_id", "compile_id", "file_type"],
    right_on=["genid", "param_id", "compile_id", "file_type"],
)
timed_df.sort_values(by=["genid", "param_id", "parsed_time"])
```
```{python}
def reduce_col(inference_col):
    def concat(x, y):
        t = az.InferenceData(
            posterior=xr.concat([x.posterior, y.posterior], dim="group")
        )
        return t

    return reduce(concat, inference_col)


def concat_over_sliced_MI(
    df_MI: pd.DataFrame,
    aggr_dims=["genid", "param_id", "file_type", "model_dir", "model_name"],
    data_col="inference_data",
    time_col="parsed_time",
):
    df_aggr = df_MI.groupby(aggr_dims).agg(
        pooled=pd.NamedAgg(column=data_col, aggfunc=reduce_col),
        time_mean=pd.NamedAgg(column=time_col, aggfunc="mean"),
        time_std=pd.NamedAgg(column=time_col, aggfunc="std"),
    )
    return df_aggr


new_df = concat_over_sliced_MI(timed_df).reset_index()
new_df
```
## Make seaborn plot
```{python}
df_sns = new_df.set_index(["genid", "param_id"]).sort_values(
    ["file_type", "model_dir", "model_name"], ascending=False
)
base_colors = sns.color_palette("Set1", n_colors=3)
cs_reject = sns.light_palette(base_colors[2], n_colors=10, reverse=True)
cs_unif = sns.light_palette(base_colors[1], n_colors=10, reverse=True)
c_rb = "k"  # , base_colors[2]

hierarchical_palette = {
    ("tppl", "independence", "reject-accept"): cs_reject[0],
    ("tppl", "rb-emul", "reject-accept"): cs_reject[4],
    ("tppl", "independence", "uniformization"): cs_unif[0],
    ("tppl", "rb-emul", "uniformization"): cs_unif[4],
    ("rb", "rb", "rb"): c_rb,
}
h_style = {
    ("tppl", "independence", "reject-accept"): {"linestyle": "--"},
    ("tppl", "independence", "uniformization"): {"linestyle": ":"},
    ("tppl", "rb-emul", "reject-accept"): {"linestyle": "--"},
    ("tppl", "rb-emul", "uniformization"): {"linestyle": ":"},
    ("rb", "rb", "rb"): {},
}
vars = ["mu", "beta", "lambda_01", "lambda_10", "lambda_12", "lambda_21"]
vars_to_tex = {
    "mu": r"$\mu$",
    "beta": r"$\beta$",
    "lambda_01": r"$\lambda_{01}$",
    "lambda_10": r"$\lambda_{10}$",
    "lambda_12": r"$\lambda_{12}$",
    "lambda_21": r"$\lambda_{21}$",
}


def gen_fig_axd():
    gs_kw = dict(width_ratios=[1] * 6, height_ratios=[1] * 3)
    fig = plt.figure(figsize=(12, 10))
    gs = GridSpec(
        nrows=3,
        ncols=6,
        height_ratios=[1, 1, 1],
        width_ratios=[1] * 6,
        hspace=0.3,
        wspace=0.5,
    )
    axd = {
        "mu": fig.add_subplot(gs[0, 0:3]),
        "beta": fig.add_subplot(gs[0, 3:]),
        "lambda_01": fig.add_subplot(gs[1, 0:2]),
        "lambda_10": fig.add_subplot(gs[1, 2:4]),
        "lambda_12": fig.add_subplot(gs[2, 0:2]),
        "lambda_21": fig.add_subplot(gs[2, 2:4]),
    }
    return fig, axd


def get_label(fp, md, mn):
    return f"RevBayes" if fp == "rb" else f"TreePPL:\n {md}, {mn}"
```
```{python}
def conv_stats(df):
    def calc_ess(inf_data, method):
        chains = az.InferenceData(
            posterior=inf_data.posterior.rename({"group": "chain", "chain": "group"})
        )
        ess = az.ess(chains, method=method)
        return ess.to_dataframe().squeeze()

    def calc_rhat(inf_data):
        chains = az.InferenceData(
            posterior=inf_data.posterior.rename({"group": "chain", "chain": "group"})
        )

        rhat = az.rhat(chains)
        return rhat.to_dataframe().squeeze()

    stats = ["ess", "ess_per_sample"]
    stats = ["ess"]
    df_bulk_ps = df[["pooled", "time_mean"]].apply(
        lambda r: calc_ess(r["pooled"], "bulk") / r["time_mean"], axis=1
    )
    df_tail_ps = df[["pooled", "time_mean"]].apply(
        lambda r: calc_ess(r["pooled"], "tail") / r["time_mean"], axis=1
    )
    df_rhat = df["pooled"].apply(calc_rhat)
    return df_bulk_ps, df_tail_ps, df_rhat
```

```{python}

```
```{python}
df_bulk, df_tail, df_rhat = conv_stats(
    df_sns.reset_index().set_index(
        ["genid", "param_id", "file_type", "model_dir", "model_name"]
    )
)


def show_df(df):
    vars = ["mu", "beta", "lambda_01", "lambda_10", "lambda_12", "lambda_21"]
    df["min"] = df[vars].min(axis=1)
    df["max"] = df[vars].max(axis=1)
    df["median"] = df[vars].median(axis=1)
    df["mean"] = df[vars].mean(axis=1)
    df = df.drop(columns=vars)
    return df.reset_index().sort_values(
        by=["genid", "param_id", "file_type", "model_dir", "model_name"]
    )


def show_non_conv_df(rhat_df, thresh=1.01):
    vars = ["mu", "beta", "lambda_01", "lambda_10", "lambda_12", "lambda_21"]
    non_conv = rhat_df[vars].apply(lambda row: (row > thresh).any(), axis=1)
    return (
        rhat_df[non_conv]
        .reset_index()
        .sort_values(by=["genid", "param_id", "file_type", "model_dir", "model_name"])
    )


show_df(df_bulk)
```
```{python}
show_non_conv_df(df_rhat)
```
```{python}

show_df(df_tail)
```
```{python}
df_bulk
```
```{python}
show_df(df_rhat)
```

```{python}
preserve = ["file_type", "model_dir", "model_name"]
df_bulk_plot = df_bulk.sort_values(["file_type", "model_name", "model_dir"])

# Build plot
fig, axs = plt.subplots(2, 2, figsize=(10, 6))
axs = axs.flatten()
for h in [0, 1, 2, 3]:
    gen_hist_idx = h
    ax = axs[h]
    plot_data = df_bulk_plot.loc[(0, gen_hist_idx)].reset_index()
    df_long = plot_data.melt(
        id_vars=preserve, var_name="variable", value_name="ess_per_s"
    )
    df_long["group_label"] = df_long.apply(
        lambda row: tuple([row[n] for n in preserve]), axis=1
    )
    df_long["color"] = df_long["group_label"].map(lambda x: hierarchical_palette[x])
    df_long["custom_label"] = df_long["group_label"].map(lambda x: get_label(*x))
    # Get bar width and offsets
    group_width = 0.8
    n_bars = df_long["custom_label"].nunique()
    bar_width = group_width / n_bars

    variables = ["mu", "beta", "lambda_01", "lambda_10", "lambda_12", "lambda_21"]
    bar_labels = df_long["custom_label"].unique()
    for i, label in enumerate(bar_labels):
        for j, var in enumerate(variables):
            y_val = df_long.query("variable == @var and custom_label == @label")[
                "ess_per_s"
            ]
            if not y_val.empty:
                ax.bar(
                    j + i * bar_width - group_width / 2 + bar_width / 2,
                    y_val.values[0],
                    width=bar_width,
                    color=df_long.query("custom_label == @label")["color"].values[0],
                    label=(
                        label if j == 0 else ""
                    ),  # only label first occurrence for legend
                    linewidth=0,
                )
    # X-axis setup
    ax.set_xticks(range(len(variables)))
    ax.set_xticklabels([vars_to_tex[v] for v in variables])
    ax.set_xlabel("")
    ax.set_ylabel("ESS / s")
    ax.set_title(
        rf"$\mu^* = $ {data_params.loc[h, 'mu']}, "
        rf"$\beta^* = $ {data_params.loc[h, 'beta']}"
    )

fig.suptitle("ESS/s per Variable and Method-Run")

# Legend
handles, labels = ax.get_legend_handles_labels()
by_label = dict(zip(labels, handles))  # remove duplicates
ax.legend(
    by_label.values(), by_label.keys(), title="Method", bbox_to_anchor=(1.0, 1.00)
)

plt.tight_layout()
fig.savefig("ESS_p_s.pdf")
plt.show()
```




```{python}
data = df_sns.loc[(0, gen_hist_idx)]
fig, axd = gen_fig_axd()
for i, (k, row) in enumerate(data.iterrows()):
    fp = row["file_type"]
    row_data = row["pooled"].posterior.to_dataframe()
    m_key = (fp, row["model_dir"], row["model_name"])
    label = get_label(*m_key)
    color = hierarchical_palette[m_key]
    style = h_style[m_key]
    for var in vars:
        ax = axd[var]
        sns.kdeplot(
            data=row_data,
            x=var,
            color=color,
            ax=ax,
            label=label,
            legend=False,
            bw_method=0.1,
            alpha=1.0,
            **style,
        )
        # Fill
        sns.kdeplot(
            data=row_data,
            x=var,
            color=color,
            ax=ax,
            legend=False,
            alpha=0.1,
            linewidth=0,
            bw_method=0.1,
            fill=True,
        )

for var in vars:
    ax = axd[var]
    ax.set_xlabel(vars_to_tex[var], fontsize=12)
    ax.set_ylabel("")
    if var.startswith("lambda"):
        ax.set_xlim([0.0, 1.0])
    else:
        ax.set_xlim(left=0.0)
    gen_val = (
        data_params.loc[gen_hist_idx, "".join(var.split("_"))] / 100
        if var.startswith("lambda")
        else data_params.loc[gen_hist_idx, var]
    )
    ax.axvline(
        gen_val,
        color="red",
        linestyle="--",
        linewidth=2,
    )
    xticks = ax.get_xticks()
    ax.set_xticks(sorted(set(list(xticks) + [gen_val])))
    ax.tick_params(axis="x", labelsize=8)
    ax.tick_params(axis="y", labelsize=8)
axd["lambda_21"].legend(bbox_to_anchor=(1.0, 1.00))
fig.suptitle(f"Densities for generated dataset  #{gen_hist_idx}")
# fig.tight_layout()
"""
fig.subplots_adjust(
    hspace=0.4,  # vertical spacing between rows
    wspace=4.5,  # horizontal spacing between columns
    left=0.05,
    right=0.95,
    top=0.52,
    bottom=0.08,  # margins
)
"""
fig.show()
fig.savefig(proc_output.get_temp_dir(RUN_NAME) / f"densities.{gen_hist_idx}.pdf")
```
